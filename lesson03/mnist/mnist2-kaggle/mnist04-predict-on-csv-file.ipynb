{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875bcbdb-b248-454d-8f91-ffc366278ef0",
   "metadata": {},
   "source": [
    "# Predicting without converting the MNIST training images to files on disk\n",
    "\n",
    "As convenient as it may see, leveraging also code I have used before, converting the `csv`-file to `png`-files on disk feel unneccessary.\n",
    "\n",
    "It took a while to make it work, but finally here we go: In this approach, I make the predictions directly on the `csv`-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8befe84-bd03-43c8-a841-365fce78d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa11b3-4de8-4de1-8adb-e00645a9addf",
   "metadata": {},
   "source": [
    "## PIL Image vs. Fast.AI image\n",
    "\n",
    "A standard PIL image (PIL = Python Imaging Library) has the type `PIL.Image.Image`. This type, can, however, not be used im combination with `learn.predict`. It expects other types, for example, the Fast.AI interpretation of an image `fastai.vision.core.PILImage`. Therefore, the data needs to be converted into the right format before being able to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c63b6e-e9a6-47eb-ad1e-0cd774ea12ab",
   "metadata": {},
   "source": [
    "### Failure mode: Why predict is not possible\n",
    "\n",
    "Let's demonstrate the failure so that we know what we are taking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25a3dbb-0221-4859-a8a8-4f324ac6d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a5cf91-a20f-49d5-af5e-e9842c2aac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_image(index):\n",
    "    im = torch.tensor(test.iloc[index])\n",
    "    image = Image.frombytes('L', (28, 28), bytes(im))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a373bbae-7a66-458d-9db7-87f0d74d45d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBklEQVR4nGNgoD9gRDC52BkYGBySGfLvYlHX/RcCDLDI2TyBSp4/qochefUvHNw3QZd0ev73798cDY3iL3//tjGjO8jemIFh820GhrMGDAxCH3F4wPLv37/8DAwMDAxMmJIITVgkTXGYyMDAwMBwFm4sC7KwrdrfBQy6wgwMx34ju5abL+BVFoO65L9DDLLKDFc9nyJ0aJWs/YsM7pawIyTL/v79+/frlXsI6QX8cMl/f//+3ZvMoHDq79+/H6qq9v39+3cdXPL/379/3927d+/j37/P3BgYBNc//fsXLjkPZtrZLEcGBgYGBrsvc+GuZROZycDAwJD98fc3qHK+H7/wBcWgBAC12Y+lQlhU6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = get_test_image(0)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63e284a-11b8-4b9a-9e95-ee0bb188e5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fff95-61dc-4b0a-babd-8186d1f0c044",
   "metadata": {},
   "source": [
    "Here we go, we have an image of type `PIL.Image.Image`, let's try to call `learn.predict` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af28f744-b828-496c-b590-b7d0ea33fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('../mnist1-fastai/export.pkl')\n",
    "#learn.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca8285-d65f-4a08-9964-76af654ddc30",
   "metadata": {},
   "source": [
    "If you uncommented and executed the above line, the result would be an error and the following `AssertionError` would appear:\n",
    "    \n",
    "```\n",
    "AssertionError: Expected an input of type in \n",
    "  - <class 'pathlib.PosixPath'>\n",
    "  - <class 'pathlib.Path'>\n",
    "  - <class 'str'>\n",
    "  - <class 'torch.Tensor'>\n",
    "  - <class 'numpy.ndarray'>\n",
    "  - <class 'bytes'>\n",
    "  - <class 'fastai.vision.core.PILImage'>\n",
    " but got <class 'PIL.Image.Image'>\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79642e8a-0c8e-41e3-bdbb-5a750bfdbcb0",
   "metadata": {},
   "source": [
    "While other options would be possible, I tried to create an image of type `fastai.vision.core.PILImage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009dcddb-2ee5-4bc3-b249-6b934bf14138",
   "metadata": {},
   "source": [
    "## Failure mode: Converting `PIL.Image.Image`to `fastai.vision.core.PILImage`\n",
    "\n",
    "This seemingly easy task was a challenge to me. Even the forums did not help (because I did not read to the end): https://forums.fast.ai/t/how-to-convert-a-pil-image-to-a-fastai-image-object/68407/8\n",
    "\n",
    "Finally, I found a way in the docs: https://docs.fast.ai/vision.core.html#pilimagebw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff5fa89-9932-47a6-965f-ec68f4c8ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create a tensor from the image:\n",
    "timg = TensorImage(image2tensor(image))\n",
    "#timg\n",
    "\n",
    "# Step 2: Create a new image from the tensor\n",
    "#tpil = PILImage.create(timg)\n",
    "#print(type(tpil))\n",
    "#tpil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b0842-5539-403f-b4de-1ec3757fd72e",
   "metadata": {},
   "source": [
    "But still this does not work, because we get a TypeError:\n",
    "\n",
    "`TypeError: Cannot handle this data type: (1, 1, 1), |u1`\n",
    "\n",
    "The reason is that the image is stored in the wrong format. Mode `L` (as explained [here](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes)) just creates a grey-scale image and the methods expect an RBG image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506d26a3-edac-4ab0-9c4d-40f5e6336b81",
   "metadata": {},
   "source": [
    "### Solution: Converting `PIL.Image.Image`to `fastai.vision.core.PILImage`\n",
    "\n",
    "Let's solve this by converting the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d21e448e-6f40-4e1a-980b-ba2b321235d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.convert('RGB') # this is the magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050c863-6465-4574-bfbb-6f0250cf9d93",
   "metadata": {},
   "source": [
    "Now the above code runs and converts the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59871c64-5d0b-414e-8aa2-619f684d4100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.vision.core.PILImage'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB30lEQVR4nO2UvcriQBSG51sXBYWIP42FIAgaBEVQC8H/QrDzHuyiYGGwyAXYeAUWSm5ALay1ELQQBhtTSbQxChbiQBQxGLcIO+vnxuwattvvqSbvOeedc2bCAPDFv+ZDO2w2m00mE/7MZDKlUgkAUK1WeZ7XuWez2bypEQ6HdTomEonNZqNqOp/PJ5NJKBR625TjOFVHzHq9jkaj75nmcrndboctKpUKSZIkSdZqNVEUFbHRaBgMhqfCP1xUOp2ORCLKejAYLJdLZQ0hxMdqt9sRQu/1q0o8HscTWK3Wp+g3fabarek0jcVi+gq1gBBqjP/9byySyaTP57vdbizLAgCCwaDD4VBC0+lUkqSnfJXbt1gsBEEUi8X9fk9RFADA7/e7XC5ZlsfjMQDA7XZ7vV4AAMdxhUJBEAStjgKBAE3T3W5X+5/H8DxP0/Tj46BCvV5/rDmdTovFYrVaaVuzLPv7sf5ClmWcOhwOlQfJ4/HMZjOsH49HhmEYhhmNRljs9XovTe/3O847HA6rnyCEFHG73ebzeSXZZrP1+31BEJTQS9NOp/NqRgghRVHZbPapJJVKiaLYbrcfxU+3bzQanU5nq9V6FMvlMkJIkqTz+azaCkEQl8vler2+bPaL/4kfT+Gu5NolgqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "PILImage mode=RGB size=28x28"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: create a tensor from the image:\n",
    "timg = TensorImage(image2tensor(image))\n",
    "\n",
    "# Step 2: Create a new image from the tensor\n",
    "tpil = PILImage.create(timg)\n",
    "print(type(tpil))\n",
    "tpil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b1145-c65f-4e71-af81-2f2aa0ea9a2a",
   "metadata": {},
   "source": [
    "### Solution: function `pil2fast` / `pil2fast2`\n",
    "\n",
    "Let's wrap this in a function to close the topic for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "911ccb17-b52c-4ef7-be66-5fd91359e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil2fast(pil):\n",
    "    timg = TensorImage(image2tensor(pil))\n",
    "    fast = tpil = PILImage.create(timg)\n",
    "    return fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "985f3395-73e1-4c2c-91b1-9d5cfc8a52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB30lEQVR4nO2UvcriQBSG51sXBYWIP42FIAgaBEVQC8H/QrDzHuyiYGGwyAXYeAUWSm5ALay1ELQQBhtTSbQxChbiQBQxGLcIO+vnxuwattvvqSbvOeedc2bCAPDFv+ZDO2w2m00mE/7MZDKlUgkAUK1WeZ7XuWez2bypEQ6HdTomEonNZqNqOp/PJ5NJKBR625TjOFVHzHq9jkaj75nmcrndboctKpUKSZIkSdZqNVEUFbHRaBgMhqfCP1xUOp2ORCLKejAYLJdLZQ0hxMdqt9sRQu/1q0o8HscTWK3Wp+g3fabarek0jcVi+gq1gBBqjP/9byySyaTP57vdbizLAgCCwaDD4VBC0+lUkqSnfJXbt1gsBEEUi8X9fk9RFADA7/e7XC5ZlsfjMQDA7XZ7vV4AAMdxhUJBEAStjgKBAE3T3W5X+5/H8DxP0/Tj46BCvV5/rDmdTovFYrVaaVuzLPv7sf5ClmWcOhwOlQfJ4/HMZjOsH49HhmEYhhmNRljs9XovTe/3O847HA6rnyCEFHG73ebzeSXZZrP1+31BEJTQS9NOp/NqRgghRVHZbPapJJVKiaLYbrcfxU+3bzQanU5nq9V6FMvlMkJIkqTz+azaCkEQl8vler2+bPaL/4kfT+Gu5NolgqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x28>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil = get_test_image(0).convert('RGB')\n",
    "print(type(pil))\n",
    "pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc7c4a76-c060-48ba-977d-780a1fb33e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.vision.core.PILImage'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB30lEQVR4nO2UvcriQBSG51sXBYWIP42FIAgaBEVQC8H/QrDzHuyiYGGwyAXYeAUWSm5ALay1ELQQBhtTSbQxChbiQBQxGLcIO+vnxuwattvvqSbvOeedc2bCAPDFv+ZDO2w2m00mE/7MZDKlUgkAUK1WeZ7XuWez2bypEQ6HdTomEonNZqNqOp/PJ5NJKBR625TjOFVHzHq9jkaj75nmcrndboctKpUKSZIkSdZqNVEUFbHRaBgMhqfCP1xUOp2ORCLKejAYLJdLZQ0hxMdqt9sRQu/1q0o8HscTWK3Wp+g3fabarek0jcVi+gq1gBBqjP/9byySyaTP57vdbizLAgCCwaDD4VBC0+lUkqSnfJXbt1gsBEEUi8X9fk9RFADA7/e7XC5ZlsfjMQDA7XZ7vV4AAMdxhUJBEAStjgKBAE3T3W5X+5/H8DxP0/Tj46BCvV5/rDmdTovFYrVaaVuzLPv7sf5ClmWcOhwOlQfJ4/HMZjOsH49HhmEYhhmNRljs9XovTe/3O847HA6rnyCEFHG73ebzeSXZZrP1+31BEJTQS9NOp/NqRgghRVHZbPapJJVKiaLYbrcfxU+3bzQanU5nq9V6FMvlMkJIkqTz+azaCkEQl8vler2+bPaL/4kfT+Gu5NolgqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "PILImage mode=RGB size=28x28"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast = pil2fast(pil)\n",
    "print(type(fast))\n",
    "fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad46e6ab-8789-4215-98ec-aa9750e10155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil2fast2(pil):\n",
    "    return PILImage.create(np.array(pil.convert('RGB')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef3b6aeb-e76f-4994-8087-3edadb518841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.vision.core.PILImage'>\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB30lEQVR4nO2UvcriQBSG51sXBYWIP42FIAgaBEVQC8H/QrDzHuyiYGGwyAXYeAUWSm5ALay1ELQQBhtTSbQxChbiQBQxGLcIO+vnxuwattvvqSbvOeedc2bCAPDFv+ZDO2w2m00mE/7MZDKlUgkAUK1WeZ7XuWez2bypEQ6HdTomEonNZqNqOp/PJ5NJKBR625TjOFVHzHq9jkaj75nmcrndboctKpUKSZIkSdZqNVEUFbHRaBgMhqfCP1xUOp2ORCLKejAYLJdLZQ0hxMdqt9sRQu/1q0o8HscTWK3Wp+g3fabarek0jcVi+gq1gBBqjP/9byySyaTP57vdbizLAgCCwaDD4VBC0+lUkqSnfJXbt1gsBEEUi8X9fk9RFADA7/e7XC5ZlsfjMQDA7XZ7vV4AAMdxhUJBEAStjgKBAE3T3W5X+5/H8DxP0/Tj46BCvV5/rDmdTovFYrVaaVuzLPv7sf5ClmWcOhwOlQfJ4/HMZjOsH49HhmEYhhmNRljs9XovTe/3O847HA6rnyCEFHG73ebzeSXZZrP1+31BEJTQS9NOp/NqRgghRVHZbPapJJVKiaLYbrcfxU+3bzQanU5nq9V6FMvlMkJIkqTz+azaCkEQl8vler2+bPaL/4kfT+Gu5NolgqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "PILImage mode=RGB size=28x28"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast2 = pil2fast2(pil)\n",
    "print(type(fast2))\n",
    "fast2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a8985c-ffc3-4793-ac1f-1697e15dafaa",
   "metadata": {},
   "source": [
    "`pil2fast2` is the faster version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa1b63-4dd4-4939-a5f1-a3c7a187de45",
   "metadata": {},
   "source": [
    "## Back to Kaggle\n",
    "\n",
    "With all the conversion out of the way, let's apply that to the Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89d1faf5-a407-47eb-85d2-9ab2c032638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fast_test_image(index):\n",
    "    im = torch.tensor(test.iloc[index])\n",
    "    pil = Image.frombytes('L', (28, 28), bytes(im))#.convert('RGB')\n",
    "    fast = pil2fast2(pil)\n",
    "    return fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73aa650a-9376-4d8c-bb66-bf2717486a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAIAAAD9b0jDAAAB30lEQVR4nO2UvcriQBSG51sXBYWIP42FIAgaBEVQC8H/QrDzHuyiYGGwyAXYeAUWSm5ALay1ELQQBhtTSbQxChbiQBQxGLcIO+vnxuwattvvqSbvOeedc2bCAPDFv+ZDO2w2m00mE/7MZDKlUgkAUK1WeZ7XuWez2bypEQ6HdTomEonNZqNqOp/PJ5NJKBR625TjOFVHzHq9jkaj75nmcrndboctKpUKSZIkSdZqNVEUFbHRaBgMhqfCP1xUOp2ORCLKejAYLJdLZQ0hxMdqt9sRQu/1q0o8HscTWK3Wp+g3fabarek0jcVi+gq1gBBqjP/9byySyaTP57vdbizLAgCCwaDD4VBC0+lUkqSnfJXbt1gsBEEUi8X9fk9RFADA7/e7XC5ZlsfjMQDA7XZ7vV4AAMdxhUJBEAStjgKBAE3T3W5X+5/H8DxP0/Tj46BCvV5/rDmdTovFYrVaaVuzLPv7sf5ClmWcOhwOlQfJ4/HMZjOsH49HhmEYhhmNRljs9XovTe/3O847HA6rnyCEFHG73ebzeSXZZrP1+31BEJTQS9NOp/NqRgghRVHZbPapJJVKiaLYbrcfxU+3bzQanU5nq9V6FMvlMkJIkqTz+azaCkEQl8vler2+bPaL/4kfT+Gu5NolgqMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "PILImage mode=RGB size=28x28"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = get_fast_test_image(0)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e228462a-0534-43a4-b30c-ebedee7d0aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.vision.core.PILImage"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9f791d5-0601-49a2-9a11-9a8e49939d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(img):\n",
    "    pred,pred_idx,probs = learn.predict(img)\n",
    "    return dict(zip(learn.dls.vocab, map(float, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "71b7817e-3f85-4768-92a7-163b71c0dfe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'0': 6.806673127357499e-07,\n",
       " '1': 9.510736731499492e-08,\n",
       " '2': 0.9999420642852783,\n",
       " '3': 3.5234781535109505e-05,\n",
       " '4': 1.0088542268249512e-07,\n",
       " '5': 3.694026190714794e-06,\n",
       " '6': 8.713337962262813e-08,\n",
       " '7': 9.428195880900603e-06,\n",
       " '8': 4.397437351144617e-06,\n",
       " '9': 4.274736966181081e-06}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf98c276-4ab6-4b54-b482-8395eab329c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(img):\n",
    "    pred, _, _ = learn.predict(img)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d768c54-8a7f-405c-a6e5-b10e76caf764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2993f9a4-3956-4b87-b948-5062dfd11ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission():\n",
    "    submission = pd.DataFrame(columns=['ImageId', 'Label'])\n",
    "\n",
    "    image_ids = []\n",
    "    preds = []\n",
    "\n",
    "\n",
    "    for i in range(len(test)):\n",
    "#    for i in range(20):\n",
    "        test_image = get_fast_test_image(i)\n",
    "        pred = get_prediction(test_image)\n",
    "\n",
    "        image_ids.append(i+1)\n",
    "        preds.append(str(pred))   \n",
    "\n",
    "    submission['ImageId'] = image_ids\n",
    "    submission['Label'] = preds\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e6f941ab-1228-4b4b-9b68-0f2de883d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission = create_submission()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60654255-594f-4879-8510-e4b2801359ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f51dea-5b96-4b5f-827d-2fcb6918cde5",
   "metadata": {},
   "source": [
    "Not surprising, the result was the same as with the first submission:\n",
    "\n",
    "![My second submission result](sub2-099442.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
